{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stemmer for stemming\n",
    "STEMMER = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(text: str) -> str:\n",
    "    \"\"\"Function which receives raw text(str), then tokenize it with nltk,\n",
    "    removes all stopwords and punctuation marks.\n",
    "    Returns str object.\"\"\"\n",
    "    words = list()\n",
    "    text = text.lower()\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    for token in tokenized:\n",
    "        token = STEMMER.stem(token)\n",
    "        if token not in nltk.corpus.stopwords.words(\"english\") and not all(ch in string.punctuation for ch in token):\n",
    "            words.append(token)\n",
    "\n",
    "    return \" \".join(word for word in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(type_: str, files_to_read: int) -> tuple:\n",
    "    \"\"\"Function for reading data in specific folder wit hspecific datastructure.\"\"\"\n",
    "    total_files = files_to_read\n",
    "    files_to_read = int(files_to_read // 2)\n",
    "    curr_file_num = 0\n",
    "\n",
    "    # get_all_negative\n",
    "    negative_reviews = list()\n",
    "    iterator = 0\n",
    "    print(f\"Getting {type_} data.\")\n",
    "    for filename in os.listdir(f\"./aclImdb/{type_}/neg\"):\n",
    "        with open(f\"./aclImdb/{type_}/neg/\"+str(filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            negative_reviews.append(f.read())\n",
    "\n",
    "        if iterator == files_to_read:\n",
    "            break\n",
    "\n",
    "        if curr_file_num % 100 == 0:\n",
    "            print(f\"Read {curr_file_num} of {total_files} files\")\n",
    "        curr_file_num += 1\n",
    "        iterator += 1\n",
    "\n",
    "    # get_all_positive\n",
    "    positive_reviews = list()\n",
    "    iterator = 0\n",
    "    for filename in os.listdir(f\"./aclImdb/{type_}/pos\"):\n",
    "        with open(f\"./aclImdb/{type_}/pos/\"+str(filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            positive_reviews.append(f.read())\n",
    "\n",
    "        if iterator == files_to_read:\n",
    "            break\n",
    "\n",
    "        if curr_file_num % 100 == 0:\n",
    "            print(f\"Read {curr_file_num} of {total_files} files\")\n",
    "        curr_file_num += 1\n",
    "        iterator += 1\n",
    "\n",
    "    print(f\"T{type_[1:]} data successfully loaded.\")\n",
    "\n",
    "    return positive_reviews, negative_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloaad required packages\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting train data.\n",
      "Read 0 of 500 files\n",
      "Read 100 of 500 files\n",
      "Read 200 of 500 files\n",
      "Read 300 of 500 files\n",
      "Read 400 of 500 files\n",
      "Train data successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read train data from dataset\n",
    "train_num = int(input(\"Number of reviews to train >> \"))\n",
    "positive_train, negative_train = get_data(\"train\", train_num)\n",
    "# create and concat train dataframes\n",
    "df_positive_train = pd.DataFrame(positive_train, columns=[\"review\"])\n",
    "df_positive_train[\"type\"] = 1\n",
    "df_negative_train = pd.DataFrame(negative_train, columns=[\"review\"])\n",
    "df_negative_train[\"type\"] = 0\n",
    "\n",
    "# concat train dataframes\n",
    "train_reviews = pd.concat(\n",
    "    (df_positive_train, df_negative_train), axis=0).sample(frac=1.0)\n",
    "train_reviews.index = range(0, len(train_reviews))\n",
    "\n",
    "# clean train memory\n",
    "del positive_train\n",
    "del negative_train\n",
    "del df_positive_train\n",
    "del df_negative_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test data.\n",
      "Read 0 of 100 files\n",
      "Test data successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "# read test data from dataset\n",
    "test_num = int(input(\"Number of reviews to test >> \"))\n",
    "positive_test, negative_test = get_data(\"test\", test_num)\n",
    "# create and concat test dataframes\n",
    "df_positive_test = pd.DataFrame(positive_test, columns=[\"review\"])\n",
    "df_positive_test[\"type\"] = 1\n",
    "df_negative_test = pd.DataFrame(negative_test, columns=[\"review\"])\n",
    "df_negative_test[\"type\"] = 0\n",
    "\n",
    "# concat test dataframes\n",
    "test_reviews = pd.concat(\n",
    "    (df_positive_test, df_negative_test), axis=0).sample(frac=1.0)\n",
    "test_reviews.index = range(0, len(test_reviews))\n",
    "\n",
    "# clean train memory\n",
    "del positive_test\n",
    "del negative_test\n",
    "del df_positive_test\n",
    "del df_negative_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating train data.\n",
      "Train data was successfully validated.\n",
      "Validating test data\n",
      "Test data was successfully validated.\n"
     ]
    }
   ],
   "source": [
    "# tokenize + stem + stopwords (validating\\formatting text)\n",
    "print(\"Validating train data.\")\n",
    "train_reviews[\"review\"] = train_reviews[\"review\"].apply(preproc)\n",
    "print(\"Train data was successfully validated.\\n\"+\"Validating test data\")\n",
    "test_reviews[\"review\"] = test_reviews[\"review\"].apply(preproc)\n",
    "print(\"Test data was successfully validated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7647058823529411\n"
     ]
    }
   ],
   "source": [
    "vectorizer_BOW = CountVectorizer()\n",
    "train_BOW = vectorizer_BOW.fit_transform(train_reviews[\"review\"])\n",
    "test_BOW = vectorizer_BOW.transform(test_reviews[\"review\"])\n",
    "\n",
    "classifier_BOW = LogisticRegression(random_state=0).fit(\n",
    "train_BOW, train_reviews[\"type\"])\n",
    "\n",
    "prediction_BOW = classifier_BOW.predict(test_BOW)\n",
    "print(accuracy_score(prediction_BOW, test_reviews[\"type\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW with biagramms implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7647058823529411\n"
     ]
    }
   ],
   "source": [
    "vectorizer_BOW_bi = CountVectorizer(ngram_range=(1, 3))\n",
    "train_BOW_bi = vectorizer_BOW_bi.fit_transform(train_reviews[\"review\"])\n",
    "test_BOW_BI = vectorizer_BOW_bi.transform(test_reviews[\"review\"])\n",
    "\n",
    "classifier_BOW_bi = LogisticRegression(random_state=0).fit(\n",
    "    train_BOW_bi, train_reviews[\"type\"])\n",
    "\n",
    "prediction_BOW_bi = classifier_BOW_bi.predict(test_BOW_BI)\n",
    "print(accuracy_score(prediction_BOW_bi, test_reviews[\"type\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7647058823529411\n"
     ]
    }
   ],
   "source": [
    "vectorizer_TFIDF = TfidfVectorizer()\n",
    "train_TFIDF = vectorizer_TFIDF.fit_transform(train_reviews[\"review\"])\n",
    "test_TFIDF = vectorizer_TFIDF.transform(test_reviews[\"review\"])\n",
    "\n",
    "classifier_TFIDF = LogisticRegression(\n",
    "    random_state=0).fit(train_TFIDF, train_reviews[\"type\"])\n",
    "\n",
    "prediction_TFIDF = classifier_TFIDF.predict(test_TFIDF)\n",
    "print(accuracy_score(prediction_TFIDF, test_reviews[\"type\"]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c71dcc72a0dfe207550a7e83ffe04560f6ee9f148e0ad1929b2b97d9e5670a86"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
